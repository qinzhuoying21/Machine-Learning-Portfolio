# 房价预测项目学习交流总结
本次交流围绕一份开源房价预测项目展开，核心是梳理项目解题思路、验证学习理解的正确性，同时探讨项目处理步骤的细节与疑问，以下是完整的内容总结，分为**项目核心背景**、**项目完整实施流程**、**学习中的关键疑问与解答**、**项目核心发现与结论**四大板块：
## 一、项目核心背景
1. 该房价预测项目核心特征为**高维特征**（共600+），与泰坦尼克号项目差异显著，高维特征易导致模型复杂、过拟合，因此**特征处理**是项目核心第一步。
2. 学习方式为复现开源项目代码，梳理其解题思路并验证自身理解，交流中重点探讨了项目特征工程、模型构建、评估优化等环节的细节。

## 二、项目完整实施流程
项目整体遵循**标签分析→特征分析→特征工程→数据预处理→模型构建与评估→模型优化→模型集成**的流程，步骤环环相扣，具体操作如下：
### 步骤1：标签（房价）分布分析与变换
1. 绘制房价（cell price）分布柱状图，发现房价数据**右偏**，存在拖尾异常值；
2. 对房价做**LOG1P变换**（log(1+x)），既让分布更接近正态，又能安全处理0值，避免普通log变换的报错问题。

### 步骤2：特征初步分析
1. 聚焦**数值型特征**，计算其与房价的相关系数并构建矩阵，按相关性排序选取**Top10特征**（剔除房价本身后为9个），仅作为初步筛选参考，非最终特征集；
2. 分析9个高相关数值特征的分布与偏度，绘制柱状图并拟合曲线，**定量计算每个数值特征的偏度值**，评估偏斜程度。

### 步骤3：特征工程（新特征创建+偏度校正）
1. **创建有实际意义的组合特征**：总居住面积（地下室+一层+二层面积）、总卫生间数量、各类门廊面积、房屋年龄（销售年份-建造年份）、改造时间（销售年份-改建年份）、是否改建（二分类变量）；
2. **自动化偏度校正**：对偏度＞0.75的数值特征做**对数变换**，变换后重新计算偏度，大部分特征偏度降至0.几，仅少数为2-3，偏斜问题大幅缓解。

### 步骤4：训练集+测试集统一预处理
1. 先将训练集与测试集**合并**，避免分开展开特征工程导致的信息不一致，后续再按原划分切分；
2. 分别分析数值型、类别型特征的统计信息，构建**3个预处理管道+列转换器**，实现不同类型特征的统一处理：
    - 数值型管道：**中位数填充缺失值**→**鲁棒缩放**（减少异常值对尺度的影响，在对数变换后执行）；
    - 类别型管道：**众数填充缺失值**→**独热编码**（直接编码，未对高基数类别做筛选）；
    - 列转换器：合并数值、类别预处理管道，实现特征的统一预处理。
3. 按**目标变量y（房价）的长度**对合并数据集切片，切回训练特征X_train和测试特征X_test，逻辑合理且无信息泄露。

### 步骤5：多模型构建与初步评估
1. 构建**三类共10+模型**，覆盖线性、树基、现代梯度提升框架，尽可能挖掘数据规律：
    - 线性模型：基础线性回归、Ridge回归（L2惩罚）、拉索回归、弹性网络；
    - 树模型：决策树、随机森林（500棵树，取均值/投票预测）、传统梯度提升（学习率0.03，累加修正误差）；
    - 现代梯度提升框架：XGBoost、LightGBM、CatBoost（带正则化，支持并行计算，自动处理缺失值）。
2. 选用**RMSLE（均方根对数误差）**和**R²（决定系数）**作为评估指标：RMSLE越小预测越准，R²越大模型对数据的解释力越强；
3. 模型初步排名（按指标表现）：
    - 第一梯队：弹性网络、拉索回归、基础线性回归（RMSLE小，R²达0.9+，表现最优）；
    - 第二梯队：CatBoost、XGBoost、传统梯度提升；
    - 第三梯队：LightGBM、随机森林、决策树。

### 步骤6：模型优化（CV+OOF流程+超参数搜索）
1. 设计**通用5折交叉验证（CV）+OOF（Out of Fold）训练函数**，封装可复用流程，支持所有回归模型，核心是用Pipeline拼接预处理管道与模型，**避免信息泄露**；
2. 针对XGBoost做**超参数随机搜索**，提升模型性能：
    - 确定待测参数范围（树的数量、学习率等）并给出候选值；
    - 将预处理与XGBoost打包为整体，随机选25组参数做3折交叉验证；
    - 选出预测误差最小的最优参数，输出对应RMSLE评分。
3. 用**最优参数**重新执行5折CV+OOF流程，训练XGBoost模型，得到CV RMSLE评分、训练集OOF预测值、测试集预测值。

### 步骤7：模型集成与结果还原
1. 分析发现**CatBoost与XGBoost预测结果高度一致**，对二者做**加权平均集成**（权重：CatBoost0.75、XGBoost0.25），得到最终Blender预测结果；
2. 对集成结果做**指数反变换**，将LOG1P变换后的预测值还原为原始房价尺度，完成最终预测。

## 三、学习中的关键疑问与解答
交流中针对项目操作细节提出多个疑问，均得到明确解答，核心疑问及答案如下：
1. **鲁棒缩放与对数变换的关系**：鲁棒缩放在对数变换后执行，对数变换负责**降低特征偏度、让分布接近正态**，鲁棒缩放负责**减少残留异常值对特征尺度的影响**，二者配合提升数据质量；
2. **合并数据集后切分的逻辑**：因目标变量y仅训练集有，故按y的长度作为切片点，能精准将合并数据集切回原训练/测试特征集，无信息泄露；
3. **现代梯度提升框架与树模型的关系**：XGBoost、LightGBM、CatBoost并非独立模型，而是**对传统树基模型的优化**，加入正则化惩罚树的复杂度，支持并行计算、自动处理缺失值，性能优于传统梯度提升/随机森林；
4. **集成选择CatBoost/XGBoost而非最优线性模型的原因**：线性模型虽表现好，但仅能捕捉**线性关系**；房价预测存在复杂非线性模式与特征交互，CatBoost/XGBoost作为树基集成模型，对非线性关系的捕捉能力更强，且CatBoost能内置处理类别特征，适配独热编码后的高维问题；
5. **特征处理的完整逻辑**：并非仅筛选/压缩，需先处理缺失值/异常值，再做筛选/降维，否则会影响后续处理效果。

## 四、项目核心发现与结论
1. **意外核心发现**：600+高维特征下，**线性模型表现优于多数复杂树基模型**，说明该房价数据中**特征与房价的线性关系极强**，是数据本身的核心规律；
2. **特征工程的重要性**：创建有实际意义的组合特征，能大幅提升模型解释力，比原始单一特征更能反映房屋价值的核心影响因素；
3. **预处理的规范性**：训练集+测试集统一预处理、Pipeline拼接预处理与模型，是避免信息泄露、保证模型泛化能力的关键；
4. **模型集成的价值**：对预测结果高度一致的优秀模型做加权平均，能进一步提升预测稳定性，减少单一模型的误差；
5. **高维特征处理的注意点**：独热编码对高基数类别易导致**维度爆炸**，虽本项目未做筛选，但后续可优化（如保留高频类别），或配合正则化缓解过拟合。

## 五、项目可优化方向
结合交流内容，该开源项目仍有部分可改进的细节，后续学习中可尝试优化：
1. 类别特征处理：对高基数类别做筛选（保留高频类别），或改用目标编码，避免独热编码的维度爆炸问题；
2. 偏度校正：对偏度极高的特征，改用Box-Cox变换，替代统一的对数变换，提升校正效果；
3. 模型集成：尝试加入线性模型做多模型集成，结合线性模型的强解释力与树基模型的非线性捕捉能力，可能进一步提升性能；
4. 特征筛选：除相关性分析外，可增加方差过滤、随机森林特征重要性分析，进一步精简高维特征，降低模型复杂度。